{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e795560c-16fc-4710-a548-dd0d868a4a7f",
   "metadata": {},
   "source": [
    "<h1> Spirometer classification </h1>\n",
    "\n",
    "This python notebook transforms the input data available in the github repository and test it with multiple classification algorithms to find out which on fits better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca942d-2978-47ad-98af-dffe91f94248",
   "metadata": {},
   "source": [
    "<h3> Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bf16df-9014-418a-a2bc-19b7482797d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for using pandas daraframe\n",
    "import numpy as np # for som math operations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib import cm\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler # for standardizing the Data\n",
    "from sklearn.decomposition import PCA # for PCA calculation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from pycm import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8f06c1-dc34-4988-a869-171525c9d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV data file load\n",
    "audio_features = pd.read_csv(\"Audios/Caracteristiques_20211004.csv\",sep=\";\",decimal=',')\n",
    "\n",
    "#Values transformation into numeric values\n",
    "audio_features['sex'].mask(audio_features['sex'] == 'H', 1, inplace=True)\n",
    "audio_features['sex'].mask(audio_features['sex'] == 'M', 0, inplace=True)\n",
    "\n",
    "audio_features[\"sex\"] = audio_features[\"sex\"].astype(np.float64)\n",
    "audio_features[\"age\"] = audio_features[\"age\"].astype(np.float64)\n",
    "\n",
    "audio_features =audio_features.drop(\"Inf_Mut\",axis=1)\n",
    "\n",
    "#Remove outliers\n",
    "audio_features.drop(audio_features[audio_features[\"age\"]>74].index, inplace=True)\n",
    "\n",
    "#Join age features in groups\n",
    "audio_features['age'].mask(audio_features['age'] < 20, 1, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(25) & audio_features['age'].ge(20), 2, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(30) & audio_features['age'].ge(25), 3, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(35) & audio_features['age'].ge(30), 4, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(40) & audio_features['age'].ge(35), 5, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(45) & audio_features['age'].ge(40), 6, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(50) & audio_features['age'].ge(45), 7, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(55) & audio_features['age'].ge(50), 8, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(60) & audio_features['age'].ge(55), 9, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(65) & audio_features['age'].ge(60), 10, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(70) & audio_features['age'].ge(65), 11, inplace=True)\n",
    "audio_features['age'].mask(audio_features['age'].lt(75) & audio_features['age'].ge(70), 12, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b73f3-5f87-41d0-99d8-f22f2d171925",
   "metadata": {},
   "source": [
    "<h3> Corellation </h3>\n",
    "In this section we check the correlation 1 by 1 feature as a diagnostic when checking other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9b663c-84d9-416c-b4e4-3be70c934516",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_matrix = audio_features.loc[:, audio_features.columns != 'age'].corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d1c2b5-7061-4bd4-bb81-0e60edffa0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_Cres1</th>\n",
       "      <th>f_Cres2</th>\n",
       "      <th>f_Cres3</th>\n",
       "      <th>f_Cres4</th>\n",
       "      <th>f_Cres5</th>\n",
       "      <th>f_Cres6</th>\n",
       "      <th>f_Cres7</th>\n",
       "      <th>Enr_Bn1</th>\n",
       "      <th>Enr_Bn2</th>\n",
       "      <th>Enr_Bn3</th>\n",
       "      <th>Enr_Bn4</th>\n",
       "      <th>Enr_Bn5</th>\n",
       "      <th>Enr_Bn6</th>\n",
       "      <th>Enr_Bn7</th>\n",
       "      <th>f_Med1</th>\n",
       "      <th>f_Med2</th>\n",
       "      <th>f_Med3</th>\n",
       "      <th>f_Med4</th>\n",
       "      <th>f_Med5</th>\n",
       "      <th>f_Med6</th>\n",
       "      <th>f_Med7</th>\n",
       "      <th>IE_Bn1</th>\n",
       "      <th>IE_Bn2</th>\n",
       "      <th>IE_Bn3</th>\n",
       "      <th>IE_Bn4</th>\n",
       "      <th>IE_Bn5</th>\n",
       "      <th>IE_Bn6</th>\n",
       "      <th>IE_Bn7</th>\n",
       "      <th>H_tf</th>\n",
       "      <th>H_t</th>\n",
       "      <th>H_f</th>\n",
       "      <th>fm</th>\n",
       "      <th>kurt_Mgt</th>\n",
       "      <th>MomC_11</th>\n",
       "      <th>MomC_77</th>\n",
       "      <th>MomC_1515</th>\n",
       "      <th>MomM_11</th>\n",
       "      <th>MomM_77</th>\n",
       "      <th>MomM_1515</th>\n",
       "      <th>sex</th>\n",
       "      <th>max_db</th>\n",
       "      <th>total_db</th>\n",
       "      <th>total_db_1_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f_Cres1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024547</td>\n",
       "      <td>0.096480</td>\n",
       "      <td>0.081980</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>0.139975</td>\n",
       "      <td>0.136188</td>\n",
       "      <td>0.274204</td>\n",
       "      <td>0.358709</td>\n",
       "      <td>0.278845</td>\n",
       "      <td>0.127198</td>\n",
       "      <td>0.070878</td>\n",
       "      <td>0.058840</td>\n",
       "      <td>0.396105</td>\n",
       "      <td>0.777010</td>\n",
       "      <td>0.029479</td>\n",
       "      <td>0.218207</td>\n",
       "      <td>0.182467</td>\n",
       "      <td>0.093156</td>\n",
       "      <td>0.080088</td>\n",
       "      <td>0.169936</td>\n",
       "      <td>0.235277</td>\n",
       "      <td>0.571545</td>\n",
       "      <td>0.518226</td>\n",
       "      <td>0.326053</td>\n",
       "      <td>0.214478</td>\n",
       "      <td>0.068338</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>0.202861</td>\n",
       "      <td>0.200071</td>\n",
       "      <td>0.261931</td>\n",
       "      <td>0.396690</td>\n",
       "      <td>0.013303</td>\n",
       "      <td>0.031087</td>\n",
       "      <td>0.134577</td>\n",
       "      <td>0.217801</td>\n",
       "      <td>0.033281</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>0.070752</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.142983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Cres2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075614</td>\n",
       "      <td>0.149475</td>\n",
       "      <td>0.023243</td>\n",
       "      <td>0.020610</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.241658</td>\n",
       "      <td>0.202005</td>\n",
       "      <td>0.311574</td>\n",
       "      <td>0.231075</td>\n",
       "      <td>0.166435</td>\n",
       "      <td>0.054033</td>\n",
       "      <td>0.146053</td>\n",
       "      <td>0.043162</td>\n",
       "      <td>0.779631</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.067569</td>\n",
       "      <td>0.028277</td>\n",
       "      <td>0.015411</td>\n",
       "      <td>0.331011</td>\n",
       "      <td>0.167631</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.267455</td>\n",
       "      <td>0.306537</td>\n",
       "      <td>0.240439</td>\n",
       "      <td>0.210401</td>\n",
       "      <td>0.159179</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.247282</td>\n",
       "      <td>0.041749</td>\n",
       "      <td>0.114890</td>\n",
       "      <td>0.075179</td>\n",
       "      <td>0.061765</td>\n",
       "      <td>0.057473</td>\n",
       "      <td>0.018424</td>\n",
       "      <td>0.019425</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.020184</td>\n",
       "      <td>0.085999</td>\n",
       "      <td>0.071661</td>\n",
       "      <td>0.085063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Cres3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.257811</td>\n",
       "      <td>0.188796</td>\n",
       "      <td>0.079085</td>\n",
       "      <td>0.288014</td>\n",
       "      <td>0.135283</td>\n",
       "      <td>0.538611</td>\n",
       "      <td>0.155284</td>\n",
       "      <td>0.360042</td>\n",
       "      <td>0.532632</td>\n",
       "      <td>0.375775</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.770123</td>\n",
       "      <td>0.145135</td>\n",
       "      <td>0.125897</td>\n",
       "      <td>0.113666</td>\n",
       "      <td>0.389309</td>\n",
       "      <td>0.262665</td>\n",
       "      <td>0.278750</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>0.273749</td>\n",
       "      <td>0.394195</td>\n",
       "      <td>0.380151</td>\n",
       "      <td>0.264687</td>\n",
       "      <td>0.339340</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.327401</td>\n",
       "      <td>0.293005</td>\n",
       "      <td>0.072602</td>\n",
       "      <td>0.114805</td>\n",
       "      <td>0.173606</td>\n",
       "      <td>0.128853</td>\n",
       "      <td>0.078241</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.105645</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.167958</td>\n",
       "      <td>0.021010</td>\n",
       "      <td>0.010343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Cres4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035815</td>\n",
       "      <td>0.066247</td>\n",
       "      <td>0.232819</td>\n",
       "      <td>0.139169</td>\n",
       "      <td>0.303429</td>\n",
       "      <td>0.140384</td>\n",
       "      <td>0.271746</td>\n",
       "      <td>0.303370</td>\n",
       "      <td>0.325830</td>\n",
       "      <td>0.047889</td>\n",
       "      <td>0.022872</td>\n",
       "      <td>0.066227</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>0.729885</td>\n",
       "      <td>0.104628</td>\n",
       "      <td>0.068324</td>\n",
       "      <td>0.360073</td>\n",
       "      <td>0.045461</td>\n",
       "      <td>0.033567</td>\n",
       "      <td>0.060829</td>\n",
       "      <td>0.313603</td>\n",
       "      <td>0.417717</td>\n",
       "      <td>0.393128</td>\n",
       "      <td>0.292507</td>\n",
       "      <td>0.402755</td>\n",
       "      <td>0.078706</td>\n",
       "      <td>0.400095</td>\n",
       "      <td>0.165031</td>\n",
       "      <td>0.052012</td>\n",
       "      <td>0.004262</td>\n",
       "      <td>0.073328</td>\n",
       "      <td>0.047924</td>\n",
       "      <td>0.028055</td>\n",
       "      <td>0.025008</td>\n",
       "      <td>0.027439</td>\n",
       "      <td>0.082481</td>\n",
       "      <td>0.170696</td>\n",
       "      <td>0.066171</td>\n",
       "      <td>0.049610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Cres5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077768</td>\n",
       "      <td>0.282188</td>\n",
       "      <td>0.180308</td>\n",
       "      <td>0.026186</td>\n",
       "      <td>0.153556</td>\n",
       "      <td>0.035234</td>\n",
       "      <td>0.128579</td>\n",
       "      <td>0.249692</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>0.110973</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.123186</td>\n",
       "      <td>0.126740</td>\n",
       "      <td>0.747320</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.353278</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.047749</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>0.094354</td>\n",
       "      <td>0.201932</td>\n",
       "      <td>0.085855</td>\n",
       "      <td>0.071396</td>\n",
       "      <td>0.202687</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.120830</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.052567</td>\n",
       "      <td>0.067198</td>\n",
       "      <td>0.040525</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>0.031697</td>\n",
       "      <td>0.067155</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.204408</td>\n",
       "      <td>0.150841</td>\n",
       "      <td>0.072906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Cres6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071969</td>\n",
       "      <td>0.013592</td>\n",
       "      <td>0.107743</td>\n",
       "      <td>0.286984</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>0.107575</td>\n",
       "      <td>0.115241</td>\n",
       "      <td>0.514994</td>\n",
       "      <td>0.252780</td>\n",
       "      <td>0.056873</td>\n",
       "      <td>0.082882</td>\n",
       "      <td>0.016557</td>\n",
       "      <td>0.085345</td>\n",
       "      <td>0.720799</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>0.253701</td>\n",
       "      <td>0.324661</td>\n",
       "      <td>0.449036</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.360314</td>\n",
       "      <td>0.007151</td>\n",
       "      <td>0.038656</td>\n",
       "      <td>0.186257</td>\n",
       "      <td>0.064709</td>\n",
       "      <td>0.196833</td>\n",
       "      <td>0.459353</td>\n",
       "      <td>0.020902</td>\n",
       "      <td>0.012486</td>\n",
       "      <td>0.108724</td>\n",
       "      <td>0.194837</td>\n",
       "      <td>0.038572</td>\n",
       "      <td>0.020081</td>\n",
       "      <td>0.029553</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.153558</td>\n",
       "      <td>0.107164</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Cres7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.340833</td>\n",
       "      <td>0.344922</td>\n",
       "      <td>0.048182</td>\n",
       "      <td>0.246521</td>\n",
       "      <td>0.346396</td>\n",
       "      <td>0.422540</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.190323</td>\n",
       "      <td>0.212116</td>\n",
       "      <td>0.293112</td>\n",
       "      <td>0.207535</td>\n",
       "      <td>0.300454</td>\n",
       "      <td>0.104108</td>\n",
       "      <td>0.793362</td>\n",
       "      <td>0.323783</td>\n",
       "      <td>0.122715</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.229397</td>\n",
       "      <td>0.396691</td>\n",
       "      <td>0.491521</td>\n",
       "      <td>0.246018</td>\n",
       "      <td>0.392475</td>\n",
       "      <td>0.031056</td>\n",
       "      <td>0.418487</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>0.088987</td>\n",
       "      <td>0.140232</td>\n",
       "      <td>0.168286</td>\n",
       "      <td>0.162176</td>\n",
       "      <td>0.076070</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.023643</td>\n",
       "      <td>0.041326</td>\n",
       "      <td>0.307609</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.029264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enr_Bn1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.259034</td>\n",
       "      <td>0.210402</td>\n",
       "      <td>0.248534</td>\n",
       "      <td>0.241825</td>\n",
       "      <td>0.088310</td>\n",
       "      <td>0.229822</td>\n",
       "      <td>0.355954</td>\n",
       "      <td>0.128232</td>\n",
       "      <td>0.096515</td>\n",
       "      <td>0.100053</td>\n",
       "      <td>0.060011</td>\n",
       "      <td>0.469572</td>\n",
       "      <td>0.312837</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.252124</td>\n",
       "      <td>0.268155</td>\n",
       "      <td>0.274378</td>\n",
       "      <td>0.205977</td>\n",
       "      <td>0.061222</td>\n",
       "      <td>0.453519</td>\n",
       "      <td>0.169616</td>\n",
       "      <td>0.498394</td>\n",
       "      <td>0.114953</td>\n",
       "      <td>0.131149</td>\n",
       "      <td>0.031627</td>\n",
       "      <td>0.139185</td>\n",
       "      <td>0.117397</td>\n",
       "      <td>0.132431</td>\n",
       "      <td>0.161808</td>\n",
       "      <td>0.039923</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.054510</td>\n",
       "      <td>0.386464</td>\n",
       "      <td>0.237378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enr_Bn2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112398</td>\n",
       "      <td>0.378892</td>\n",
       "      <td>0.501808</td>\n",
       "      <td>0.546887</td>\n",
       "      <td>0.384081</td>\n",
       "      <td>0.389281</td>\n",
       "      <td>0.157367</td>\n",
       "      <td>0.661693</td>\n",
       "      <td>0.328391</td>\n",
       "      <td>0.117975</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.351580</td>\n",
       "      <td>0.407133</td>\n",
       "      <td>0.650110</td>\n",
       "      <td>0.297045</td>\n",
       "      <td>0.123159</td>\n",
       "      <td>0.321382</td>\n",
       "      <td>0.452011</td>\n",
       "      <td>0.311329</td>\n",
       "      <td>0.335593</td>\n",
       "      <td>0.235568</td>\n",
       "      <td>0.294597</td>\n",
       "      <td>0.685262</td>\n",
       "      <td>0.095228</td>\n",
       "      <td>0.022190</td>\n",
       "      <td>0.326956</td>\n",
       "      <td>0.319106</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>0.061187</td>\n",
       "      <td>0.245516</td>\n",
       "      <td>0.262177</td>\n",
       "      <td>0.159229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enr_Bn3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028140</td>\n",
       "      <td>0.274051</td>\n",
       "      <td>0.328589</td>\n",
       "      <td>0.347559</td>\n",
       "      <td>0.199507</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.158457</td>\n",
       "      <td>0.291870</td>\n",
       "      <td>0.209496</td>\n",
       "      <td>0.181198</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.201132</td>\n",
       "      <td>0.363764</td>\n",
       "      <td>0.571400</td>\n",
       "      <td>0.286506</td>\n",
       "      <td>0.047533</td>\n",
       "      <td>0.089530</td>\n",
       "      <td>0.084170</td>\n",
       "      <td>0.106738</td>\n",
       "      <td>0.101442</td>\n",
       "      <td>0.120905</td>\n",
       "      <td>0.438441</td>\n",
       "      <td>0.028388</td>\n",
       "      <td>0.048181</td>\n",
       "      <td>0.115988</td>\n",
       "      <td>0.143085</td>\n",
       "      <td>0.021093</td>\n",
       "      <td>0.040286</td>\n",
       "      <td>0.039808</td>\n",
       "      <td>0.024219</td>\n",
       "      <td>0.194380</td>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.120008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enr_Bn4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.252796</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.163737</td>\n",
       "      <td>0.075388</td>\n",
       "      <td>0.279089</td>\n",
       "      <td>0.433118</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.171724</td>\n",
       "      <td>0.103664</td>\n",
       "      <td>0.332310</td>\n",
       "      <td>0.206070</td>\n",
       "      <td>0.154575</td>\n",
       "      <td>0.054542</td>\n",
       "      <td>0.435454</td>\n",
       "      <td>0.360305</td>\n",
       "      <td>0.221649</td>\n",
       "      <td>0.106628</td>\n",
       "      <td>0.265506</td>\n",
       "      <td>0.132557</td>\n",
       "      <td>0.232479</td>\n",
       "      <td>0.019574</td>\n",
       "      <td>0.050792</td>\n",
       "      <td>0.041419</td>\n",
       "      <td>0.105064</td>\n",
       "      <td>0.097195</td>\n",
       "      <td>0.017247</td>\n",
       "      <td>0.071385</td>\n",
       "      <td>0.053987</td>\n",
       "      <td>0.039416</td>\n",
       "      <td>0.088520</td>\n",
       "      <td>0.057628</td>\n",
       "      <td>0.058671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enr_Bn5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.311940</td>\n",
       "      <td>0.107738</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.161694</td>\n",
       "      <td>0.532191</td>\n",
       "      <td>0.343904</td>\n",
       "      <td>0.161059</td>\n",
       "      <td>0.255954</td>\n",
       "      <td>0.405632</td>\n",
       "      <td>0.272446</td>\n",
       "      <td>0.241356</td>\n",
       "      <td>0.021475</td>\n",
       "      <td>0.299408</td>\n",
       "      <td>0.528196</td>\n",
       "      <td>0.411538</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.398949</td>\n",
       "      <td>0.096845</td>\n",
       "      <td>0.389741</td>\n",
       "      <td>0.261673</td>\n",
       "      <td>0.012481</td>\n",
       "      <td>0.071145</td>\n",
       "      <td>0.200890</td>\n",
       "      <td>0.194005</td>\n",
       "      <td>0.048063</td>\n",
       "      <td>0.082846</td>\n",
       "      <td>0.056253</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.196201</td>\n",
       "      <td>0.074944</td>\n",
       "      <td>0.122162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enr_Bn6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.124773</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.014266</td>\n",
       "      <td>0.417274</td>\n",
       "      <td>0.325249</td>\n",
       "      <td>0.334488</td>\n",
       "      <td>0.071883</td>\n",
       "      <td>0.553978</td>\n",
       "      <td>0.225940</td>\n",
       "      <td>0.256439</td>\n",
       "      <td>0.094951</td>\n",
       "      <td>0.185086</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.569314</td>\n",
       "      <td>0.409203</td>\n",
       "      <td>0.498476</td>\n",
       "      <td>0.109764</td>\n",
       "      <td>0.566805</td>\n",
       "      <td>0.559207</td>\n",
       "      <td>0.183013</td>\n",
       "      <td>0.039470</td>\n",
       "      <td>0.225184</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.051834</td>\n",
       "      <td>0.013074</td>\n",
       "      <td>0.013466</td>\n",
       "      <td>0.075490</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.077028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enr_Bn7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572083</td>\n",
       "      <td>0.133085</td>\n",
       "      <td>0.125467</td>\n",
       "      <td>0.052905</td>\n",
       "      <td>0.133750</td>\n",
       "      <td>0.403187</td>\n",
       "      <td>0.165593</td>\n",
       "      <td>0.520277</td>\n",
       "      <td>0.757779</td>\n",
       "      <td>0.756997</td>\n",
       "      <td>0.628031</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>0.100028</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.209439</td>\n",
       "      <td>0.261358</td>\n",
       "      <td>0.287407</td>\n",
       "      <td>0.850767</td>\n",
       "      <td>0.077417</td>\n",
       "      <td>0.028187</td>\n",
       "      <td>0.283978</td>\n",
       "      <td>0.371275</td>\n",
       "      <td>0.068905</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.154235</td>\n",
       "      <td>0.122005</td>\n",
       "      <td>0.252276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Med1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103771</td>\n",
       "      <td>0.148511</td>\n",
       "      <td>0.116044</td>\n",
       "      <td>0.015983</td>\n",
       "      <td>0.137055</td>\n",
       "      <td>0.277201</td>\n",
       "      <td>0.327829</td>\n",
       "      <td>0.666526</td>\n",
       "      <td>0.613096</td>\n",
       "      <td>0.466385</td>\n",
       "      <td>0.374395</td>\n",
       "      <td>0.163622</td>\n",
       "      <td>0.065772</td>\n",
       "      <td>0.284666</td>\n",
       "      <td>0.274842</td>\n",
       "      <td>0.371835</td>\n",
       "      <td>0.508753</td>\n",
       "      <td>0.023222</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.221142</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.058731</td>\n",
       "      <td>0.034439</td>\n",
       "      <td>0.014892</td>\n",
       "      <td>0.177211</td>\n",
       "      <td>0.162893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Med2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.012432</td>\n",
       "      <td>0.018446</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.311588</td>\n",
       "      <td>0.300951</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>0.277431</td>\n",
       "      <td>0.304132</td>\n",
       "      <td>0.216235</td>\n",
       "      <td>0.142587</td>\n",
       "      <td>0.071595</td>\n",
       "      <td>0.232903</td>\n",
       "      <td>0.053481</td>\n",
       "      <td>0.254538</td>\n",
       "      <td>0.040519</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>0.042047</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>0.056976</td>\n",
       "      <td>0.037075</td>\n",
       "      <td>0.047472</td>\n",
       "      <td>0.176968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Med3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.150844</td>\n",
       "      <td>0.072023</td>\n",
       "      <td>0.155473</td>\n",
       "      <td>0.379449</td>\n",
       "      <td>0.290927</td>\n",
       "      <td>0.405460</td>\n",
       "      <td>0.055893</td>\n",
       "      <td>0.306347</td>\n",
       "      <td>0.441687</td>\n",
       "      <td>0.428448</td>\n",
       "      <td>0.314149</td>\n",
       "      <td>0.376249</td>\n",
       "      <td>0.144875</td>\n",
       "      <td>0.352684</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.018055</td>\n",
       "      <td>0.069955</td>\n",
       "      <td>0.218141</td>\n",
       "      <td>0.218181</td>\n",
       "      <td>0.092767</td>\n",
       "      <td>0.091257</td>\n",
       "      <td>0.089226</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.105207</td>\n",
       "      <td>0.111101</td>\n",
       "      <td>0.021186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Med4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.162591</td>\n",
       "      <td>0.066894</td>\n",
       "      <td>0.264981</td>\n",
       "      <td>0.121280</td>\n",
       "      <td>0.189611</td>\n",
       "      <td>0.161886</td>\n",
       "      <td>0.188258</td>\n",
       "      <td>0.390169</td>\n",
       "      <td>0.406374</td>\n",
       "      <td>0.302555</td>\n",
       "      <td>0.309031</td>\n",
       "      <td>0.173626</td>\n",
       "      <td>0.283361</td>\n",
       "      <td>0.270466</td>\n",
       "      <td>0.055553</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.083483</td>\n",
       "      <td>0.077735</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>0.011079</td>\n",
       "      <td>0.012977</td>\n",
       "      <td>0.043386</td>\n",
       "      <td>0.168141</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.067330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Med5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.317045</td>\n",
       "      <td>0.217296</td>\n",
       "      <td>0.189126</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.153001</td>\n",
       "      <td>0.051959</td>\n",
       "      <td>0.267572</td>\n",
       "      <td>0.128832</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.138602</td>\n",
       "      <td>0.062278</td>\n",
       "      <td>0.255126</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.060252</td>\n",
       "      <td>0.048004</td>\n",
       "      <td>0.048299</td>\n",
       "      <td>0.122848</td>\n",
       "      <td>0.130655</td>\n",
       "      <td>0.152342</td>\n",
       "      <td>0.109717</td>\n",
       "      <td>0.226502</td>\n",
       "      <td>0.127922</td>\n",
       "      <td>0.028491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Med6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063091</td>\n",
       "      <td>0.106532</td>\n",
       "      <td>0.141255</td>\n",
       "      <td>0.236378</td>\n",
       "      <td>0.283930</td>\n",
       "      <td>0.278186</td>\n",
       "      <td>0.056140</td>\n",
       "      <td>0.224229</td>\n",
       "      <td>0.067208</td>\n",
       "      <td>0.055475</td>\n",
       "      <td>0.069230</td>\n",
       "      <td>0.310863</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.090356</td>\n",
       "      <td>0.049218</td>\n",
       "      <td>0.023042</td>\n",
       "      <td>0.032355</td>\n",
       "      <td>0.021336</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.016076</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>0.074952</td>\n",
       "      <td>0.116216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_Med7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.233342</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>0.256343</td>\n",
       "      <td>0.474475</td>\n",
       "      <td>0.616920</td>\n",
       "      <td>0.692973</td>\n",
       "      <td>0.392291</td>\n",
       "      <td>0.601498</td>\n",
       "      <td>0.044633</td>\n",
       "      <td>0.659962</td>\n",
       "      <td>0.179823</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.061620</td>\n",
       "      <td>0.174779</td>\n",
       "      <td>0.138497</td>\n",
       "      <td>0.026439</td>\n",
       "      <td>0.060473</td>\n",
       "      <td>0.047087</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.312312</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.085761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE_Bn1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.748785</td>\n",
       "      <td>0.566947</td>\n",
       "      <td>0.347871</td>\n",
       "      <td>0.165905</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.153831</td>\n",
       "      <td>0.115662</td>\n",
       "      <td>0.032768</td>\n",
       "      <td>0.135634</td>\n",
       "      <td>0.600357</td>\n",
       "      <td>0.066099</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.321054</td>\n",
       "      <td>0.398489</td>\n",
       "      <td>0.110201</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>0.263714</td>\n",
       "      <td>0.077616</td>\n",
       "      <td>0.045842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE_Bn2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.835090</td>\n",
       "      <td>0.521959</td>\n",
       "      <td>0.287007</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>0.073262</td>\n",
       "      <td>0.223990</td>\n",
       "      <td>0.258143</td>\n",
       "      <td>0.301871</td>\n",
       "      <td>0.813305</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.035908</td>\n",
       "      <td>0.371469</td>\n",
       "      <td>0.444527</td>\n",
       "      <td>0.081373</td>\n",
       "      <td>0.032619</td>\n",
       "      <td>0.035507</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>0.238613</td>\n",
       "      <td>0.175218</td>\n",
       "      <td>0.188641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE_Bn3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.803091</td>\n",
       "      <td>0.556896</td>\n",
       "      <td>0.244792</td>\n",
       "      <td>0.211232</td>\n",
       "      <td>0.516168</td>\n",
       "      <td>0.198090</td>\n",
       "      <td>0.582937</td>\n",
       "      <td>0.682108</td>\n",
       "      <td>0.060840</td>\n",
       "      <td>0.028068</td>\n",
       "      <td>0.263309</td>\n",
       "      <td>0.361726</td>\n",
       "      <td>0.041499</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>0.086243</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>0.184033</td>\n",
       "      <td>0.150759</td>\n",
       "      <td>0.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE_Bn4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865359</td>\n",
       "      <td>0.535172</td>\n",
       "      <td>0.422266</td>\n",
       "      <td>0.731271</td>\n",
       "      <td>0.059833</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.090422</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.108034</td>\n",
       "      <td>0.202079</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.057163</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.054465</td>\n",
       "      <td>0.084552</td>\n",
       "      <td>0.207934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE_Bn5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.773577</td>\n",
       "      <td>0.558634</td>\n",
       "      <td>0.770499</td>\n",
       "      <td>0.024425</td>\n",
       "      <td>0.810019</td>\n",
       "      <td>0.077935</td>\n",
       "      <td>0.084240</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.062086</td>\n",
       "      <td>0.087940</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.029368</td>\n",
       "      <td>0.021006</td>\n",
       "      <td>0.065699</td>\n",
       "      <td>0.087994</td>\n",
       "      <td>0.167308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE_Bn6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.782672</td>\n",
       "      <td>0.641608</td>\n",
       "      <td>0.029721</td>\n",
       "      <td>0.697548</td>\n",
       "      <td>0.236832</td>\n",
       "      <td>0.115212</td>\n",
       "      <td>0.076499</td>\n",
       "      <td>0.065679</td>\n",
       "      <td>0.038469</td>\n",
       "      <td>0.154860</td>\n",
       "      <td>0.059462</td>\n",
       "      <td>0.038290</td>\n",
       "      <td>0.071790</td>\n",
       "      <td>0.211802</td>\n",
       "      <td>0.021313</td>\n",
       "      <td>0.010322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE_Bn7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.514249</td>\n",
       "      <td>0.091385</td>\n",
       "      <td>0.541938</td>\n",
       "      <td>0.208575</td>\n",
       "      <td>0.131264</td>\n",
       "      <td>0.105380</td>\n",
       "      <td>0.046881</td>\n",
       "      <td>0.077740</td>\n",
       "      <td>0.225842</td>\n",
       "      <td>0.109794</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.051284</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>0.046950</td>\n",
       "      <td>0.071439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_tf</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.235885</td>\n",
       "      <td>0.947788</td>\n",
       "      <td>0.122854</td>\n",
       "      <td>0.135335</td>\n",
       "      <td>0.026076</td>\n",
       "      <td>0.138378</td>\n",
       "      <td>0.053605</td>\n",
       "      <td>0.034204</td>\n",
       "      <td>0.075672</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>0.026722</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.205157</td>\n",
       "      <td>0.191638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_t</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>0.220704</td>\n",
       "      <td>0.532446</td>\n",
       "      <td>0.086713</td>\n",
       "      <td>0.236477</td>\n",
       "      <td>0.253234</td>\n",
       "      <td>0.050764</td>\n",
       "      <td>0.081329</td>\n",
       "      <td>0.065543</td>\n",
       "      <td>0.031004</td>\n",
       "      <td>0.045279</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.094618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_f</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075240</td>\n",
       "      <td>0.055458</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.069109</td>\n",
       "      <td>0.021988</td>\n",
       "      <td>0.004671</td>\n",
       "      <td>0.038718</td>\n",
       "      <td>0.044340</td>\n",
       "      <td>0.040045</td>\n",
       "      <td>0.056324</td>\n",
       "      <td>0.194966</td>\n",
       "      <td>0.199720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.383060</td>\n",
       "      <td>0.436904</td>\n",
       "      <td>0.085418</td>\n",
       "      <td>0.018687</td>\n",
       "      <td>0.057513</td>\n",
       "      <td>0.038214</td>\n",
       "      <td>0.272051</td>\n",
       "      <td>0.091420</td>\n",
       "      <td>0.213943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurt_Mgt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059778</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.011258</td>\n",
       "      <td>0.013219</td>\n",
       "      <td>0.005414</td>\n",
       "      <td>0.071434</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.254627</td>\n",
       "      <td>0.041472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomC_11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.491216</td>\n",
       "      <td>0.218672</td>\n",
       "      <td>0.206701</td>\n",
       "      <td>0.204944</td>\n",
       "      <td>0.212458</td>\n",
       "      <td>0.097894</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.047035</td>\n",
       "      <td>0.144075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomC_77</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833134</td>\n",
       "      <td>0.215563</td>\n",
       "      <td>0.191416</td>\n",
       "      <td>0.091884</td>\n",
       "      <td>0.057175</td>\n",
       "      <td>0.167627</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.013188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomC_1515</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156460</td>\n",
       "      <td>0.082948</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.066974</td>\n",
       "      <td>0.120631</td>\n",
       "      <td>0.062466</td>\n",
       "      <td>0.010793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomM_11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625053</td>\n",
       "      <td>0.540947</td>\n",
       "      <td>0.038037</td>\n",
       "      <td>0.036548</td>\n",
       "      <td>0.067321</td>\n",
       "      <td>0.017798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomM_77</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.880171</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.068809</td>\n",
       "      <td>0.018448</td>\n",
       "      <td>0.073616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomM_1515</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.080611</td>\n",
       "      <td>0.037684</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.044961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.168759</td>\n",
       "      <td>0.044114</td>\n",
       "      <td>0.134304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_db</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156602</td>\n",
       "      <td>0.444085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_db</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_db_1_sec</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(upper_tri.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c95dd6b-aab9-4eb2-a210-82e88ba97b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables to drop: ['H_f']\n"
     ]
    }
   ],
   "source": [
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.90)]\n",
    "print(\"Variables to drop:\",to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae73225-a50f-4ad5-a14f-0ed77402713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features=audio_features.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad476d1e-6a05-4d01-92e6-46d9b6414eb3",
   "metadata": {},
   "source": [
    "<h3> PCA </h3>\n",
    "We apply the PCA algorithm do determine the most important components of all de data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b59619-eaa2-478a-b772-f64ce8d8b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = audio_features.filter(audio_features.columns[:])\n",
    "df = df.loc[:, df.columns != 'age']\n",
    "\n",
    "X = df.values # getting all values as a matrix of dataframe \n",
    "sc = StandardScaler() # creating a StandardScaler object\n",
    "X_std = sc.fit_transform(X) # standardizing the data\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9878de81-e383-4947-9f20-721d3ad4abd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6298d9-c27e-4dca-926a-3dde827b8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = len(df.columns)\n",
    "pca = PCA(num_components)  \n",
    "X_pca = pca.fit_transform(X_std) # fit and reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa19e735-7d78-4bd1-b2b9-cfc3e8fb9537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 0.999)\n",
    "X_pca = pca.fit_transform(X_std) # this will fit and reduce dimensions\n",
    "print(pca.n_components_) # one can print and see how many components are selected. In this case it is 4 same as above we saw in step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fabfd09d-8c0d-4dce-bdc5-a2c5d02dc24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17980027, 0.16968025, 0.07142519, 0.06564169, 0.05445411,\n",
       "       0.04983188, 0.04474501, 0.03652538, 0.03569004, 0.03531003,\n",
       "       0.02725706, 0.02608772, 0.02264205, 0.01843313, 0.01735092,\n",
       "       0.01502149, 0.0138275 , 0.01269719, 0.01204683, 0.01097538,\n",
       "       0.0104578 , 0.00945748, 0.00726989, 0.00648963, 0.00623693,\n",
       "       0.00530318, 0.00506101, 0.00472655, 0.00405349, 0.00373675,\n",
       "       0.00304838, 0.00276667, 0.00263466, 0.00217399, 0.00180829,\n",
       "       0.00165054, 0.00145424, 0.00099836, 0.00075607])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dbe5d4-e974-45fb-b6d0-d3022ba3c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['f_Cres1', 'f_Cres2', 'f_Cres3', 'f_Cres4', 'f_Cres5', 'f_Cres6',\n",
      "       'f_Cres7', 'Enr_Bn1', 'Enr_Bn2', 'Enr_Bn3', 'Enr_Bn4', 'Enr_Bn5',\n",
      "       'Enr_Bn6', 'Enr_Bn7', 'f_Med1', 'f_Med2', 'f_Med3', 'f_Med4', 'f_Med5',\n",
      "       'f_Med6', 'f_Med7', 'IE_Bn1', 'IE_Bn2', 'IE_Bn3', 'IE_Bn4', 'IE_Bn5',\n",
      "       'IE_Bn6', 'IE_Bn7', 'H_tf', 'H_t', 'fm', 'kurt_Mgt', 'MomC_11',\n",
      "       'MomC_77', 'MomC_1515', 'MomM_11', 'MomM_77', 'MomM_1515', 'sex',\n",
      "       'max_db', 'total_db', 'total_db_1_sec'],\n",
      "      dtype='object')\n",
      "['Enr_Bn2' 'Enr_Bn3' 'Enr_Bn5' 'Enr_Bn6' 'H_t' 'H_tf' 'IE_Bn2' 'IE_Bn3'\n",
      " 'IE_Bn5' 'IE_Bn7' 'MomC_11' 'MomC_77' 'MomM_11' 'MomM_77' 'f_Cres2'\n",
      " 'f_Cres3' 'f_Cres4' 'f_Cres5' 'f_Cres6' 'f_Med1' 'f_Med2' 'f_Med4'\n",
      " 'f_Med5' 'f_Med6' 'f_Med7' 'kurt_Mgt' 'max_db' 'sex' 'total_db']\n"
     ]
    }
   ],
   "source": [
    "n_pcs= pca.n_components_ # get number of component\n",
    "# get the index of the most important feature on EACH component\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = df.columns\n",
    "print(initial_feature_names)\n",
    "# get the most important feature names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "most_important_names = np.unique(most_important_names)\n",
    "print(np.unique(most_important_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e63cc8-3f63-4080-97d3-ebe2c50c0d9a",
   "metadata": {},
   "source": [
    "<h3> SMOTE </h3>\n",
    "This algorithm is applied in order to improve and balance de data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba4da8d-8075-4757-8915-14f0351db0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9.0: 41, 2.0: 20, 3.0: 19, 5.0: 18, 6.0: 15, 4.0: 14, 7.0: 14, 8.0: 14, 10.0: 12, 11.0: 11, 1.0: 10})\n",
      "Counter({1.0: 41, 2.0: 41, 3.0: 41, 4.0: 41, 5.0: 41, 6.0: 41, 7.0: 41, 8.0: 41, 9.0: 41, 10.0: 41, 11.0: 41})\n"
     ]
    }
   ],
   "source": [
    "y = audio_features['age']\n",
    "\n",
    "feature_names = audio_features.filter(most_important_names)\n",
    "X = audio_features.filter(feature_names)\n",
    "\n",
    "print(Counter(y))\n",
    "\n",
    "oversample = SMOTE(random_state=42,k_neighbors=1)\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "counter=Counter(y)\n",
    "\n",
    "print(counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4f8de9f-0915-494a-b22a-d1ab2394d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4ec71d5-1cbb-4c9a-80a0-471bf20c26e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "54,87\t \t 60,26 \t\t 95,5\n",
      "==============================\n",
      "SVC\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "4,425\t \t 9,091 \t\t 90,91\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "59,29\t \t 65,69 \t\t 95,9\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "78,76\t \t 81,39 \t\t 97,89\n",
      "==============================\n",
      "AdaBoostClassifier\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "16,81\t \t 20,73 \t\t 91,77\n",
      "==============================\n",
      "GradientBoostingClassifier\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "63,72\t \t 66,25 \t\t 96,38\n",
      "==============================\n",
      "GaussianNB\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "42,48\t \t 47,11 \t\t 94,24\n",
      "==============================\n",
      "LogisticRegression\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "38,05\t \t 43,87 \t\t 93,91\n",
      "==============================\n",
      "LinearDiscriminantAnalysis\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "50,44\t \t 53,86 \t\t 95,05\n",
      "==============================\n",
      "QuadraticDiscriminantAnalysis\n",
      "Accuracy\tSpecificity\tSensitivity\n",
      "94,69\t \t 94,45 \t\t 99,45\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "'''\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(max_iter=10000),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    pred = clf.predict(X_test)\n",
    "    precision,recall,fscore,support=score(y_test,pred,average='macro')\n",
    "    \n",
    "    mcm=multilabel_confusion_matrix(y_test, pred)\n",
    "    tn = mcm[:, 0, 0]\n",
    "    tp = mcm[:, 1, 1]\n",
    "    fn = mcm[:, 1, 0]\n",
    "    fp = mcm[:, 0, 1]\n",
    "    \n",
    "    print(\"Accuracy\\tSpecificity\\tSensitivity\")\n",
    "    print(\"{:.4}\\t\".format(acc*100).replace('.',','),\"\\t\",\"{:.4}\".format(recall*100).replace('.',','),\"\\t\\t\",\"{:.4}\".format(((tn / (tn + fp))*100).mean()).replace('.',','))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345c469-a5f0-4230-adaf-10dc16bb7a27",
   "metadata": {},
   "source": [
    "<h3> Logistic Regresion </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0258f807-53ad-4fa9-ae41-4df47df79b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.53\n",
      "Accuracy of Logistic regression classifier on test set: 0.38\n",
      "[[7 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 4 0 4 2 1 1 0 1 2 1]\n",
      " [0 0 2 2 4 0 1 0 1 0 0]\n",
      " [0 0 0 4 0 0 0 0 1 1 1]\n",
      " [0 0 0 1 1 2 0 0 0 2 1]\n",
      " [0 0 0 1 1 4 0 0 0 0 0]\n",
      " [0 2 0 2 0 0 6 0 0 1 3]\n",
      " [3 0 1 0 1 3 0 4 0 4 0]\n",
      " [3 0 2 1 1 2 1 1 1 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 4 1]\n",
      " [1 1 0 1 0 0 0 0 0 0 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.50      0.78      0.61         9\n",
      "         2.0       0.50      0.25      0.33        16\n",
      "         3.0       0.40      0.20      0.27        10\n",
      "         4.0       0.25      0.57      0.35         7\n",
      "         5.0       0.10      0.14      0.12         7\n",
      "         6.0       0.33      0.67      0.44         6\n",
      "         7.0       0.60      0.43      0.50        14\n",
      "         8.0       0.80      0.25      0.38        16\n",
      "         9.0       0.25      0.07      0.11        14\n",
      "        10.0       0.25      0.80      0.38         5\n",
      "        11.0       0.46      0.67      0.55         9\n",
      "\n",
      "    accuracy                           0.38       113\n",
      "   macro avg       0.40      0.44      0.37       113\n",
      "weighted avg       0.45      0.38      0.36       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train, y_train)\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'.format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "\n",
    "pred = logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc4537-6ce5-444c-b218-624976e78b0f",
   "metadata": {},
   "source": [
    "<h3> Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91b07485-d608-40fd-bf09-aef62beb3e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\n",
      "Accuracy of Decision Tree classifier on test set: 0.60\n",
      "Accuracy: 0.6017699115044248\n",
      "[[ 8  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  1  1  1  0  2  1  2]\n",
      " [ 0  1  6  0  1  0  1  1  0  0  0]\n",
      " [ 0  0  0  6  0  0  0  0  0  1  0]\n",
      " [ 0  1  0  1  4  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  1  5  0  0  0  0  0]\n",
      " [ 2  0  0  1  1  0  9  0  0  1  0]\n",
      " [ 0  1  1  0  0  1  1 10  1  1  0]\n",
      " [ 1  1  2  0  2  3  1  2  0  1  1]\n",
      " [ 1  0  0  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.67      0.89      0.76         9\n",
      "         2.0       0.62      0.50      0.55        16\n",
      "         3.0       0.67      0.60      0.63        10\n",
      "         4.0       0.67      0.86      0.75         7\n",
      "         5.0       0.40      0.57      0.47         7\n",
      "         6.0       0.50      0.83      0.62         6\n",
      "         7.0       0.69      0.64      0.67        14\n",
      "         8.0       0.77      0.62      0.69        16\n",
      "         9.0       0.00      0.00      0.00        14\n",
      "        10.0       0.44      0.80      0.57         5\n",
      "        11.0       0.73      0.89      0.80         9\n",
      "\n",
      "    accuracy                           0.60       113\n",
      "   macro avg       0.56      0.66      0.59       113\n",
      "weighted avg       0.56      0.60      0.57       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22422a22-cc9f-4652-8ab0-0a333a798034",
   "metadata": {},
   "source": [
    "<h3> Random Forests </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87815278-c23e-46be-ae2a-15f56d172f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on training set: 0.61\n",
      "Accuracy of GNB classifier on test set: 0.34\n",
      "[[9 0 0 0 0 0 0 0 0 0 0]\n",
      " [4 0 1 2 3 1 1 0 1 1 2]\n",
      " [0 0 3 0 2 2 1 0 0 0 2]\n",
      " [1 0 0 4 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 2 2 0 0 1 2 0]\n",
      " [0 0 0 0 1 4 0 0 1 0 0]\n",
      " [3 0 0 0 2 3 4 0 0 1 1]\n",
      " [4 0 2 0 0 1 1 1 0 4 3]\n",
      " [0 0 1 1 1 5 2 0 1 3 0]\n",
      " [1 0 0 0 0 0 0 0 0 4 0]\n",
      " [2 0 0 0 1 0 0 0 0 0 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.38      1.00      0.55         9\n",
      "         2.0       0.00      0.00      0.00        16\n",
      "         3.0       0.43      0.30      0.35        10\n",
      "         4.0       0.57      0.57      0.57         7\n",
      "         5.0       0.17      0.29      0.21         7\n",
      "         6.0       0.22      0.67      0.33         6\n",
      "         7.0       0.44      0.29      0.35        14\n",
      "         8.0       1.00      0.06      0.12        16\n",
      "         9.0       0.25      0.07      0.11        14\n",
      "        10.0       0.24      0.80      0.36         5\n",
      "        11.0       0.43      0.67      0.52         9\n",
      "\n",
      "    accuracy                           0.34       113\n",
      "   macro avg       0.37      0.43      0.32       113\n",
      "weighted avg       0.40      0.34      0.27       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(class_weight=\"balanced\",n_estimators=100, max_depth=2, random_state=0)\n",
    "RF.fit(X_train, y_train)\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(RF.score(X_train, y_train)))\n",
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(RF.score(X_test, y_test)))\n",
    "\n",
    "pred = RF.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef46921-ca47-4385-a0b2-aace0dd1b598",
   "metadata": {},
   "source": [
    "<h3> K-Nearest Neighbors </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b7e6b6b-2413-4ff4-a31f-e4ded7673e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.73\n",
      "Accuracy of K-NN classifier on test set: 0.55\n",
      "[[ 8  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  4  3  1  6  1  0  0  0  0  0]\n",
      " [ 0  1  9  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  1  4  0  0  0  0  0  1  0]\n",
      " [ 0  0  1  1  4  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  5  0  0  0  0  1]\n",
      " [ 1  1  0  0  1  1  7  1  0  1  1]\n",
      " [ 0  0  1  0  1  0  3 10  0  1  0]\n",
      " [ 1  2  3  1  1  2  1  3  0  0  0]\n",
      " [ 1  0  0  1  0  0  0  0  0  3  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.62      0.89      0.73         9\n",
      "         2.0       0.44      0.25      0.32        16\n",
      "         3.0       0.47      0.90      0.62        10\n",
      "         4.0       0.50      0.57      0.53         7\n",
      "         5.0       0.31      0.57      0.40         7\n",
      "         6.0       0.56      0.83      0.67         6\n",
      "         7.0       0.64      0.50      0.56        14\n",
      "         8.0       0.71      0.62      0.67        16\n",
      "         9.0       0.00      0.00      0.00        14\n",
      "        10.0       0.43      0.60      0.50         5\n",
      "        11.0       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.55       113\n",
      "   macro avg       0.50      0.60      0.53       113\n",
      "weighted avg       0.50      0.55      0.50       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))\n",
    "\n",
    "pred = knn.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2c856-4847-415d-be63-9b2bcf9a16cd",
   "metadata": {},
   "source": [
    "<h3> Linear Discriminant Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99e92365-7f6f-41a5-8b98-6b1bcb1966e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA classifier on training set: 0.66\n",
      "Accuracy of LDA classifier on test set: 0.50\n",
      "[[ 8  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  7  0  4  1  0  0  0  3  0  1]\n",
      " [ 0  0  3  1  3  1  0  0  2  0  0]\n",
      " [ 0  0  0  3  0  0  0  1  2  1  0]\n",
      " [ 0  0  1  1  3  0  0  0  1  1  0]\n",
      " [ 0  0  0  0  0  4  0  0  2  0  0]\n",
      " [ 0  5  1  0  0  0  6  0  1  1  0]\n",
      " [ 1  0  0  0  2  1  0 10  1  1  0]\n",
      " [ 0  0  3  1  2  2  0  1  2  2  1]\n",
      " [ 1  0  0  0  0  0  0  0  0  4  0]\n",
      " [ 1  0  0  0  0  0  1  0  0  0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.89      0.80         9\n",
      "         2.0       0.58      0.44      0.50        16\n",
      "         3.0       0.38      0.30      0.33        10\n",
      "         4.0       0.30      0.43      0.35         7\n",
      "         5.0       0.27      0.43      0.33         7\n",
      "         6.0       0.50      0.67      0.57         6\n",
      "         7.0       0.86      0.43      0.57        14\n",
      "         8.0       0.83      0.62      0.71        16\n",
      "         9.0       0.13      0.14      0.14        14\n",
      "        10.0       0.40      0.80      0.53         5\n",
      "        11.0       0.78      0.78      0.78         9\n",
      "\n",
      "    accuracy                           0.50       113\n",
      "   macro avg       0.52      0.54      0.51       113\n",
      "weighted avg       0.56      0.50      0.51       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X_train, y_train)))\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test, y_test)))\n",
    "\n",
    "pred = lda.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100270d3-edfd-40f2-98d8-7483932585b9",
   "metadata": {},
   "source": [
    "<h3> Gaussian Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "955ebd96-162a-49a5-96df-d87443bf4969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on training set: 0.51\n",
      "Accuracy of GNB classifier on test set: 0.42\n",
      "[[ 9  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  5  1  3  2  1  0  1  0  0  0]\n",
      " [ 0  1  2  0  0  3  0  0  4  0  0]\n",
      " [ 0  0  2  2  0  1  0  1  0  1  0]\n",
      " [ 1  0  0  1  4  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  4  0  0  0  0  0]\n",
      " [ 1  4  2  0  0  3  2  1  0  0  1]\n",
      " [ 0  2  1  0  0  2  0 11  0  0  0]\n",
      " [ 0  3  1  2  4  1  1  1  1  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  4  0]\n",
      " [ 0  2  2  0  0  0  0  0  1  0  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.60      1.00      0.75         9\n",
      "         2.0       0.29      0.31      0.30        16\n",
      "         3.0       0.18      0.20      0.19        10\n",
      "         4.0       0.25      0.29      0.27         7\n",
      "         5.0       0.33      0.57      0.42         7\n",
      "         6.0       0.25      0.67      0.36         6\n",
      "         7.0       0.67      0.14      0.24        14\n",
      "         8.0       0.73      0.69      0.71        16\n",
      "         9.0       0.17      0.07      0.10        14\n",
      "        10.0       0.80      0.80      0.80         5\n",
      "        11.0       0.80      0.44      0.57         9\n",
      "\n",
      "    accuracy                           0.42       113\n",
      "   macro avg       0.46      0.47      0.43       113\n",
      "weighted avg       0.46      0.42      0.40       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))\n",
    "\n",
    "pred = gnb.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d59560-b366-495c-8402-584d8eb21aff",
   "metadata": {},
   "source": [
    "<h3> Support Vector Machine </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e7e1310-2c4a-4884-ab5e-23898bf8eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.72\n",
      "Accuracy of SVM classifier on test set: 0.54\n",
      "[[8 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 1 3 3 1 0 0 1 1 1]\n",
      " [0 0 6 0 2 0 0 0 2 0 0]\n",
      " [0 0 0 6 0 0 0 0 0 0 1]\n",
      " [0 0 2 1 3 0 0 0 0 1 0]\n",
      " [0 0 0 1 1 4 0 0 0 0 0]\n",
      " [2 1 0 0 1 1 6 0 2 1 0]\n",
      " [1 0 3 0 1 2 0 9 0 0 0]\n",
      " [1 0 2 2 1 3 1 0 3 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 4 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.57      0.89      0.70         9\n",
      "         2.0       0.71      0.31      0.43        16\n",
      "         3.0       0.40      0.60      0.48        10\n",
      "         4.0       0.46      0.86      0.60         7\n",
      "         5.0       0.25      0.43      0.32         7\n",
      "         6.0       0.36      0.67      0.47         6\n",
      "         7.0       0.86      0.43      0.57        14\n",
      "         8.0       1.00      0.56      0.72        16\n",
      "         9.0       0.38      0.21      0.27        14\n",
      "        10.0       0.50      0.80      0.62         5\n",
      "        11.0       0.78      0.78      0.78         9\n",
      "\n",
      "    accuracy                           0.54       113\n",
      "   macro avg       0.57      0.59      0.54       113\n",
      "weighted avg       0.62      0.54      0.54       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_test, y_test)))\n",
    "\n",
    "pred = svm.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec55d7-6453-4ea4-9eae-e92be2601de9",
   "metadata": {},
   "source": [
    "<h3> QDA </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "679c8669-0e1a-448f-a0ad-72e15bded869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of QDA classifier on training set: 1.00\n",
      "Accuracy of QDA classifier on test set: 0.95\n",
      "[[ 9  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 14  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 12  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0 16  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         9\n",
      "         2.0       1.00      0.88      0.93        16\n",
      "         3.0       1.00      1.00      1.00        10\n",
      "         4.0       1.00      1.00      1.00         7\n",
      "         5.0       1.00      0.86      0.92         7\n",
      "         6.0       1.00      1.00      1.00         6\n",
      "         7.0       1.00      0.86      0.92        14\n",
      "         8.0       1.00      1.00      1.00        16\n",
      "         9.0       0.70      1.00      0.82        14\n",
      "        10.0       1.00      0.80      0.89         5\n",
      "        11.0       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.95       113\n",
      "   macro avg       0.97      0.94      0.95       113\n",
      "weighted avg       0.96      0.95      0.95       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "qda = QDA()\n",
    "qda.fit(X_train, y_train)\n",
    "print('Accuracy of QDA classifier on training set: {:.2f}'\n",
    "     .format(qda.score(X_train, y_train)))\n",
    "print('Accuracy of QDA classifier on test set: {:.2f}'\n",
    "     .format(qda.score(X_test, y_test)))\n",
    "\n",
    "pred = qda.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff50bda-0359-4bdc-a4e3-c609a3111a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
